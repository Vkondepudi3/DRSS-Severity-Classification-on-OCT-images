{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecd34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hpaceice1/vkondepudi3/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#including all the libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c3e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing pre processing\n",
    "LABELS_Severity = {35: 0, 43: 0, 47: 1, 53: 1, 61: 2, 65: 2, 71: 2, 85: 2}\n",
    "mean = (.1706)\n",
    "std = (.2112)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(size=(224,224)),transforms.ToTensor(),normalize,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ea41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, args, subset='train', transform=None,):\n",
    "        if subset == 'train':\n",
    "            self.annot = pd.read_csv(args.annot_train_prime)\n",
    "        elif subset == 'test':\n",
    "            self.annot = pd.read_csv(args.annot_test_prime)\n",
    "            \n",
    "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)] \n",
    "        # print(self.annot)\n",
    "        self.root = os.path.expanduser(args.data_root)\n",
    "        self.transform = transform\n",
    "        # self.subset = subset\n",
    "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
    "        self.path_list = self.annot['File_Path'].values\n",
    "        self._labels = self.annot['Severity_Label'].values\n",
    "        assert len(self.path_list) == len(self._labels)\n",
    "        # idx_each_class = [[] for i in range(self.nb_classes)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = Image.open(self.root+self.path_list[index]).convert(\"L\"), self._labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookArgs:\n",
    "    def __init__(self, annot_train_prime = 'df_prime_train.csv', annot_test_prime = 'df_prime_test.csv', data_root = '/storage/home/hpaceice1/shared-classes/materials/ece8803fml/'):\n",
    "        self.annot_train_prime = annot_train_prime\n",
    "        self.annot_test_prime = annot_test_prime\n",
    "        self.data_root = data_root\n",
    "args = NotebookArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a70d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "trainset = OCTDataset(args, 'train', transform=transform)\n",
    "testset = OCTDataset(args, 'test', transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ec6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [01:34<00:00,  7.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_X_y_from_loader(loader):\n",
    "    X, y = [], []\n",
    "    i=0\n",
    "    for sample in tqdm(loader, total=len(loader)):\n",
    "        i = i+1\n",
    "        images, labels = sample[0], sample[1]\n",
    "        X.extend([a.numpy()[0] for a in images])\n",
    "        y.extend([a.numpy().flatten() for a in labels])\n",
    "        #break\n",
    "        #if(i == 1000):\n",
    "           #break\n",
    "    return X,y\n",
    "X_train, y_train = get_X_y_from_loader(train_loader)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf98db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9999/24252 [01:38<02:19, 102.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#performing sift on train set\n",
    "X_train_sift = []\n",
    "i=0\n",
    "for i in tqdm(range(len(X_train))):   \n",
    "    #X_hog = X_train[i].reshape((224,224))\n",
    "    img_uint8 = cv2.convertScaleAbs(X_train[i])\n",
    "    sift = cv2.SIFT_create(nfeatures=50, contrastThreshold=0.001, edgeThreshold=5)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_uint8, None)\n",
    "    descriptors=descriptors.flatten()\n",
    "    descriptors= descriptors[:6400] \n",
    "    X_train_sift.append(descriptors)\n",
    "    i =i+1\n",
    "    if i==10000:\n",
    "        break\n",
    "print(X_train_sift[0].shape)\n",
    "print(len(X_train_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9f95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24252\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "X_subset = X_train_sift\n",
    "y_subset = y_train[0:10000]\n",
    "print(len(y_subset))\n",
    "#for arr in (X_subset):\n",
    "    #print(arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d2d885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hpaceice1/vkondepudi3/.local/lib/python3.9/site-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#naive bayes implementation\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Train the Naive Bayes classifier\n",
    "clf = GaussianNB()\n",
    "clf.fit(X_subset, y_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d741020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:31<00:00,  7.91it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_X_y_from_loader(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41454061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7987/7987 [01:18<00:00, 102.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n",
      "7987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_sift = []\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    #X_hog = X_train[i].reshape((224,224))\n",
    "    img_uint8 = cv2.convertScaleAbs(X_test[i])\n",
    "    sift = cv2.SIFT_create(nfeatures=50, contrastThreshold=0.001, edgeThreshold=5)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_uint8, None)\n",
    "    descriptors = descriptors.flatten()\n",
    "    descriptors = descriptors[:6400]\n",
    "    X_test_sift.append(descriptors)\n",
    "print(X_test_sift[0].shape)\n",
    "print(len(X_test_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da7ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3483160135219732\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = clf.predict(X_test_sift)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c913db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy score: 0.3210219695818774\n",
      "Precision score: 0.387969692127719\n",
      "Recall score: 0.3483160135219732\n",
      "F1 score weighted: 0.36363330858040116\n",
      "F1 score Micro: 0.3483160135219732\n",
      "F1 score Macro: 0.3226210353298844\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate balanced accuracy score\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced accuracy score:\", balanced_acc)\n",
    "\n",
    "# Calculate precision score\n",
    "precision = precision_score(y_test, y_pred,average='weighted')\n",
    "print(\"Precision score:\", precision)\n",
    "\n",
    "# Calculate recall score\n",
    "recall = recall_score(y_test, y_pred,average='weighted')\n",
    "print(\"Recall score:\", recall)\n",
    "\n",
    "# Calculate f1 score\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "print(\"F1 score weighted:\", f1)\n",
    "\n",
    "# Calculate f1 score\n",
    "f1 = f1_score(y_test, y_pred,average='micro')\n",
    "print(\"F1 score Micro:\", f1)\n",
    "\n",
    "# Calculate f1 score\n",
    "f1 = f1_score(y_test, y_pred,average='macro')\n",
    "print(\"F1 score Macro:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c8a3bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0     1     2\n",
      "0   914   802   832\n",
      "1  1092  1551  1277\n",
      "2   424   778   317\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = [0, 1, 2]  # label names\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "print(cm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c17a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
