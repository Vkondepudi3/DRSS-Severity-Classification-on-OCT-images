{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ecd34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hpaceice1/vkondepudi3/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#including all the necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48c3e3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_Severity = {35: 0, 43: 0, 47: 1, 53: 1, 61: 2, 65: 2, 71: 2, 85: 2}\n",
    "mean = (.1706)\n",
    "std = (.2112)\n",
    "normalize = transforms.Normalize(mean=mean, std=std)\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize(size=(224,224)),transforms.ToTensor(),normalize,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ea41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, args, subset='train', transform=None,):\n",
    "        if subset == 'train':\n",
    "            self.annot = pd.read_csv(args.annot_train_prime)\n",
    "        elif subset == 'test':\n",
    "            self.annot = pd.read_csv(args.annot_test_prime)\n",
    "            \n",
    "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)] \n",
    "        # print(self.annot)\n",
    "        self.root = os.path.expanduser(args.data_root)\n",
    "        self.transform = transform\n",
    "        # self.subset = subset\n",
    "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
    "        self.path_list = self.annot['File_Path'].values\n",
    "        self._labels = self.annot['Severity_Label'].values\n",
    "        assert len(self.path_list) == len(self._labels)\n",
    "        # idx_each_class = [[] for i in range(self.nb_classes)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = Image.open(self.root+self.path_list[index]).convert(\"L\"), self._labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._labels)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64c2c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookArgs:\n",
    "    def __init__(self, annot_train_prime = 'df_prime_train.csv', annot_test_prime = 'df_prime_test.csv', data_root = '/storage/home/hpaceice1/shared-classes/materials/ece8803fml/'):\n",
    "        self.annot_train_prime = annot_train_prime\n",
    "        self.annot_test_prime = annot_test_prime\n",
    "        self.data_root = data_root\n",
    "args = NotebookArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a70d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "trainset = OCTDataset(args, 'train', transform=transform)\n",
    "testset = OCTDataset(args, 'test', transform=transform)\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ec6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 758/758 [32:53<00:00,  2.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_X_y_from_loader(loader):\n",
    "    X, y = [], []\n",
    "    i=0\n",
    "    for sample in tqdm(loader, total=len(loader)):\n",
    "        i = i+1\n",
    "        images, labels = sample[0], sample[1]\n",
    "        X.extend([a.numpy()[0] for a in images])\n",
    "        y.extend([a.numpy().flatten() for a in labels])\n",
    "        #break\n",
    "        #if(i == 1000):\n",
    "           #break\n",
    "    return X,y\n",
    "X_train, y_train = get_X_y_from_loader(train_loader)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf98db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9999/24252 [01:48<02:34, 92.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n",
      "10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#applying sift feature extraction\n",
    "X_train_sift = []\n",
    "i=0\n",
    "for i in tqdm(range(len(X_train))):   \n",
    "    #X_hog = X_train[i].reshape((224,224))\n",
    "    img_uint8 = cv2.convertScaleAbs(X_train[i])\n",
    "    sift = cv2.SIFT_create(nfeatures=50, contrastThreshold=0.001, edgeThreshold=5)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_uint8, None)\n",
    "    descriptors=descriptors.flatten()\n",
    "    descriptors= descriptors[:6400] \n",
    "    X_train_sift.append(descriptors)\n",
    "    i =i+1\n",
    "    if i==10000:\n",
    "        break\n",
    "print(X_train_sift[0].shape)\n",
    "print(len(X_train_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9f95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24252\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train))\n",
    "X_subset = X_train_sift\n",
    "y_subset = y_train[0:10000]\n",
    "print(len(y_subset))\n",
    "#for arr in (X_subset):\n",
    "    #print(arr.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d741020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [10:22<00:00,  2.49s/it]\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_X_y_from_loader(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41454061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7987/7987 [02:20<00:00, 56.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6400,)\n",
      "7987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test_sift = []\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    #X_hog = X_train[i].reshape((224,224))\n",
    "    img_uint8 = cv2.convertScaleAbs(X_test[i])\n",
    "    sift = cv2.SIFT_create(nfeatures=50, contrastThreshold=0.001, edgeThreshold=5)\n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img_uint8, None)\n",
    "    descriptors = descriptors.flatten()\n",
    "    descriptors = descriptors[:6400]\n",
    "    X_test_sift.append(descriptors)\n",
    "print(X_test_sift[0].shape)\n",
    "print(len(X_test_sift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffe7ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_subset = [i[0] for i in y_subset]\n",
    "y_test = [i[0] for i in y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57d2d885",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 0.8810282945632935\n",
      "Epoch: 2 Loss: 1.0281893014907837\n",
      "Epoch: 3 Loss: 1.0130897760391235\n",
      "Epoch: 4 Loss: 0.9264447689056396\n",
      "Epoch: 5 Loss: 0.9889447689056396\n",
      "Epoch: 6 Loss: 1.0514447689056396\n",
      "Epoch: 7 Loss: 1.1139448881149292\n",
      "Epoch: 8 Loss: 1.0514447689056396\n",
      "Epoch: 9 Loss: 1.1764447689056396\n",
      "Epoch: 10 Loss: 1.0514447689056396\n",
      "Test accuracy: 0.49079754601226994\n"
     ]
    }
   ],
   "source": [
    "#performing feed forward\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define the number of input features\n",
    "input_dim = len(X_subset[0])\n",
    "\n",
    "# Define the number of output classes\n",
    "num_classes = 3\n",
    "\n",
    "# Convert the training and test sets to PyTorch tensors\n",
    "X_train_tensor = torch.Tensor(X_subset)\n",
    "y_train_tensor = torch.LongTensor(y_subset)\n",
    "X_test_tensor = torch.Tensor(X_test_sift)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "# Define the model architecture\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dim, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024,512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512,256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, num_classes),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Define the training data loader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Epoch:', epoch+1, 'Loss:', loss.item())\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "with torch.no_grad():\n",
    "    output = model(X_test_tensor)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test)\n",
    "    print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ad9f3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "torch.Size([32, 3])\n",
      "tensor([0])\n",
      "tensor([0.4602, 0.2737, 0.2661], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(output.shape)\n",
    "print(target[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8da7ce6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on 7987 points is 0.49079754601226994\n",
      "f1 score on 7987 points is 0.3231588780327703\n",
      "Test precision on 7987 points is 0.2408822311716662\n",
      "Test recall on 7987 points is 0.49079754601226994\n",
      "Test balanced accuracy on 7987 points is 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/home/hpaceice1/vkondepudi3/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#printing all the metrics\n",
    "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(X_test_tensor)\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    predicted = predicted.cpu()\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
    "    print(f'Test accuracy on {len(predicted)} points is {accuracy}')\n",
    "    f1 = f1_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
    "    precision = precision_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
    "    recall = recall_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test_tensor.cpu().numpy(), predicted.numpy())\n",
    "    print(f'f1 score on {len(predicted)} points is {f1}')\n",
    "    print(f'Test precision on {len(predicted)} points is {precision}')\n",
    "    print(f'Test recall on {len(predicted)} points is {recall}')\n",
    "    print(f'Test balanced accuracy on {len(predicted)} points is {balanced_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
