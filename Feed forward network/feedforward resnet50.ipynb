{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EnT4QL_36W8",
        "outputId": "feccad8e-a604-4002-edd4-9ebf83c9190d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#mounting on the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOACRCh-50Ct",
        "outputId": "77734c78-9dc8-4fc5-ab65-abb8f07fa1ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/FML_Project\n"
          ]
        }
      ],
      "source": [
        "\n",
        "folder_path = '/content/drive/MyDrive/FML_Project'\n",
        "print(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU5wIs4Y6LLc",
        "outputId": "757cfdce-cee5-4f89-e2db-312414aa7e39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder exists\n",
            "/content/drive/MyDrive/FML_Project/df_prime_test.csv\n",
            "/content/drive/MyDrive/FML_Project/df_prime_train.csv\n",
            "/content/drive/MyDrive/FML_Project/Prime_FULL\n",
            "/content/drive/MyDrive/FML_Project/resnet50_features.pkl\n",
            "/content/drive/MyDrive/FML_Project/resnet50_features_testfix.pkl\n",
            "/content/drive/MyDrive/FML_Project/resnet50_features_final.pkl\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isdir(folder_path):\n",
        "    print(\"Folder exists\")\n",
        "else:\n",
        "    print(\"Folder doesn't exist\")\n",
        "\n",
        "# List all files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "    print(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JeB97hx3gWm"
      },
      "outputs": [],
      "source": [
        "class NotebookArgs:\n",
        "    def __init__(self, annot_test_prime = \"/content/drive/MyDrive/FML_Project/df_prime_test.csv\" , annot_train_prime = \"/content/drive/MyDrive/FML_Project/df_prime_train.csv\", data_root = \"/content/drive/MyDrive/FML_Project\"):\n",
        "        self.annot_train_prime = annot_train_prime\n",
        "        self.annot_test_prime = annot_test_prime\n",
        "        self.data_root = data_root\n",
        "args = NotebookArgs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wfen3WEqESwO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import argparse\n",
        "import os\n",
        "import copy\n",
        "import torch\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gyug7wCDEStQ"
      },
      "outputs": [],
      "source": [
        "LABELS_Severity = {35: 0,\n",
        "                   43: 0,\n",
        "                   47: 1,\n",
        "                   53: 1,\n",
        "                   61: 2,\n",
        "                   65: 2,\n",
        "                   71: 2,\n",
        "                   85: 2}\n",
        "mean = (.1706)\n",
        "std = (.2112)\n",
        "normalize = transforms.Normalize(mean=mean, std=std)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    lambda x : x.expand(3,*x.shape[1:]),\n",
        "    normalize,\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnVVsEOHESqc"
      },
      "outputs": [],
      "source": [
        "class OCTDataset(Dataset):\n",
        "    def __init__(self, args, subset='train', transform=None,):\n",
        "        if subset == 'train':\n",
        "            self.annot = pd.read_csv(args.annot_train_prime)\n",
        "        elif subset == 'test':\n",
        "            self.annot = pd.read_csv(args.annot_test_prime)\n",
        "            \n",
        "        self.annot['Severity_Label'] = [LABELS_Severity[drss] for drss in copy.deepcopy(self.annot['DRSS'].values)] \n",
        "        # print(self.annot)\n",
        "        self.root = os.path.expanduser(args.data_root)\n",
        "        self.transform = transform\n",
        "        # self.subset = subset\n",
        "        self.nb_classes=len(np.unique(list(LABELS_Severity.values())))\n",
        "        self.path_list = self.annot['File_Path'].values\n",
        "        self._labels = self.annot['Severity_Label'].values\n",
        "        assert len(self.path_list) == len(self._labels)\n",
        "        # idx_each_class = [[] for i in range(self.nb_classes)]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, target = Image.open(self.root+self.path_list[index]).convert(\"L\"), self._labels[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._labels)         "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnpvP8AVESng",
        "outputId": "036edc9c-651b-4831-8062-6e4b10a2a4c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "#loading the data\n",
        "trainset = OCTDataset(args, 'train', transform=transform)\n",
        "testset = OCTDataset(args, 'test', transform=transform)\n",
        "\n",
        "# Define hyperparameters\n",
        "batch_size = 128\n",
        "num_epochs = 1\n",
        "\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd-no9vaESgX"
      },
      "outputs": [],
      "source": [
        "#creating resnet50 feature extractor\n",
        "device = torch.device('cuda:0')\n",
        "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
        "model = torch.nn.Sequential(*list(resnet50.children())[:-1]).eval().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#performing resnet50 feature extraction\n",
        "def get_X_y_from_loader(loader):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for local_batch, local_labels in tqdm(loader):\n",
        "        local_batch = local_batch.to(device)\n",
        "        batched_features = model(local_batch).detach().cpu()\n",
        "        flat_features = [image.flatten().numpy() for image in batched_features]\n",
        "        flat_labels = [label.item() for label in local_labels]\n",
        "        all_features.extend(flat_features)\n",
        "        all_labels.extend(flat_labels)\n",
        "    return all_features, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, y_train = get_X_y_from_loader(train_loader)\n",
        "X_test, y_test = get_X_y_from_loader(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLAWjrYsBMEK",
        "outputId": "09707e7b-2fb5-4754-b01d-f9400ddbd86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Size : 24252 24252\n",
            "Test Size: 7987,  7987\n"
          ]
        }
      ],
      "source": [
        "# import pickle\n",
        "# with open('/content/drive/MyDrive/FML_Project/resnet50_features_final.pkl', 'rb') as f:\n",
        "#     X_train, y_train, X_test, y_test = pickle.load(f)\n",
        "# print (f\"Train Size : {len(X_train)} {len(y_train)}\")\n",
        "# print (f\"Test Size: {len(X_test)},  {len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_X2kNb96Igw",
        "outputId": "458a3eba-a6c7-4c20-d3be-9ec1570bd6e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-11-622f1f38c9b1>:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  X_train_tensor = torch.Tensor(X_train)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 Loss: 1.160139560699463 Test Accuracy: 0.49079754601226994\n",
            "Epoch: 2 Loss: 1.0203698873519897 Test Accuracy: 0.49079754601226994\n",
            "Epoch: 3 Loss: 0.9909018874168396 Test Accuracy: 0.49079754601226994\n",
            "Epoch: 4 Loss: 0.9957444667816162 Test Accuracy: 0.5169650682358833\n",
            "Epoch: 5 Loss: 0.9734517931938171 Test Accuracy: 0.5033178915738075\n",
            "Epoch: 6 Loss: 0.9358988404273987 Test Accuracy: 0.4794040315512708\n",
            "Epoch: 7 Loss: 0.8478348851203918 Test Accuracy: 0.4915487667459622\n",
            "Epoch: 8 Loss: 0.8713628649711609 Test Accuracy: 0.4601226993865031\n",
            "Epoch: 9 Loss: 0.9815295338630676 Test Accuracy: 0.4735194691373482\n",
            "Epoch: 10 Loss: 0.8620108962059021 Test Accuracy: 0.4958056842368849\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Define the number of input features\n",
        "input_dim = len(X_train[0])\n",
        "\n",
        "# Define the number of output classes\n",
        "num_classes = 3\n",
        "\n",
        "# Convert the training and test sets to PyTorch tensors\n",
        "X_train_tensor = torch.Tensor(X_train)\n",
        "y_train_tensor = torch.LongTensor(y_train)\n",
        "X_test_tensor = torch.Tensor(X_test)\n",
        "y_test_tensor = torch.LongTensor(y_test)\n",
        "\n",
        "# Define the model architecture\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(input_dim, 1024),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(1024,256),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256,64),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(64,16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, num_classes)\n",
        ")\n",
        "model = model.cuda()\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "# Define the training data loader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "#print(train_dataset)\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        #print(batch_idx)\n",
        "        data = data.cuda()\n",
        "        target = target.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # Evaluate the model on the test data\n",
        "    with torch.no_grad():\n",
        "        output = model(X_test_tensor.cuda())\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        predicted = predicted.cpu()\n",
        "        accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "        print('Epoch:', epoch+1, 'Loss:', loss.item(), 'Test Accuracy:', accuracy)\n",
        "        # print(f'Test accuracy on {len(predicted)} points is {accuracy}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmcEM-NYLsMc",
        "outputId": "1c0af0dc-f2da-4bcb-a0b8-c3fd894c2c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy on 7987 points is 0.4958056842368849\n",
            "f1 score on 7987 points is 0.4343224447707391\n",
            "Test precision on 7987 points is 0.39956063571513467\n",
            "Test recall on 7987 points is 0.4958056842368849\n",
            "Test balanced accuracy on 7987 points is 0.3834837781266353\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, balanced_accuracy_score\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(X_test_tensor.cuda())\n",
        "    _, predicted = torch.max(output.data, 1)\n",
        "    predicted = predicted.cpu()\n",
        "    accuracy = (predicted == y_test_tensor).sum().item() / len(y_test_tensor)\n",
        "    print(f'Test accuracy on {len(predicted)} points is {accuracy}')\n",
        "    f1 = f1_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
        "    precision = precision_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
        "    recall = recall_score(y_test_tensor.cpu().numpy(), predicted.numpy(), average='weighted')\n",
        "    balanced_accuracy = balanced_accuracy_score(y_test_tensor.cpu().numpy(), predicted.numpy())\n",
        "    print(f'f1 score on {len(predicted)} points is {f1}')\n",
        "    print(f'Test precision on {len(predicted)} points is {precision}')\n",
        "    print(f'Test recall on {len(predicted)} points is {recall}')\n",
        "    print(f'Test balanced accuracy on {len(predicted)} points is {balanced_accuracy}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
